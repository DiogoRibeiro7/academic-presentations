\frametitle{Example: Q-Learning with OpenAI Gym}
\begin{lstlisting}[language=Python, basicstyle=\tiny]
import gym
import numpy as np

# Create environment
env = gym.make('CartPole-v1')

# Initialize Q-table (discretized state space)
n_states = (6, 12)  # bins for position and velocity
n_actions = env.action_space.n
Q = np.zeros(n_states + (n_actions,))

# Hyperparameters
alpha = 0.1  # learning rate
gamma = 0.99  # discount factor
epsilon = 0.1  # exploration rate

def discretize_state(state):
    # Discretize continuous state
    pos_bins = np.linspace(-2.4, 2.4, n_states[0])
    vel_bins = np.linspace(-3, 3, n_states[1])
    return (np.digitize(state[0], pos_bins),
            np.digitize(state[1], vel_bins))

# Training loop
for episode in range(1000):
    state = discretize_state(env.reset())
    done = False

    while not done:
        # Epsilon-greedy action selection
        if np.random.random() < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(Q[state])

        # Take action
        next_state, reward, done, _ = env.step(action)
        next_state = discretize_state(next_state)

        # Q-learning update
        Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])
        state = next_state
  \end{lstlisting}
