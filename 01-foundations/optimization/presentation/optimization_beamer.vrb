\frametitle{Example: Training a Model with PyTorch}
\begin{lstlisting}[language=Python, basicstyle=\tiny]
import torch
import torch.nn as nn
import torch.optim as optim

# Define model, loss, optimizer
model = MyNeuralNetwork()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        # Forward pass
        output = model(data)
        loss = criterion(output, target)

        # Backward pass and optimization
        optimizer.zero_grad()  # Clear gradients
        loss.backward()        # Compute gradients

        # Gradient clipping (optional, prevents exploding gradients)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()       # Update parameters
        total_loss += loss.item()

    # Learning rate scheduling
    scheduler.step()

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for data, target in val_loader:
            output = model(data)
            val_loss += criterion(output, target).item()

    # Logging
    print(f'Epoch {epoch}: Train Loss: {total_loss/len(train_loader):.4f}, '
          f'Val Loss: {val_loss/len(val_loader):.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')
  \end{lstlisting}
