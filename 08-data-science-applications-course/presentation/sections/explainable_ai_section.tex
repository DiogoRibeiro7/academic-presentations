% ================================================================
% EXPLAINABLE AI & MODEL INTERPRETABILITY
% Comprehensive presentation on XAI methods and practices
%
% Topics Covered:
% 1. Why Interpretability Matters
% 2. Model-Specific Interpretability
% 3. Model-Agnostic Methods (SHAP, LIME)
% 4. Feature Importance and Visualization
% 5. Fairness and Trust
%
% Total: ~40 slides
% Author: Diogo Ribeiro
% Date: January 2025
% ================================================================

\section{Explainable AI \& Model Interpretability}

% ================================================================
% PART 1: FOUNDATIONS
% ================================================================

\begin{frame}[fragile]{Why Interpretability Matters}
\textbf{The black box problem in machine learning}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{The Accuracy-Interpretability Tradeoff:}

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.8]
\draw[->] (0,0) -- (6,0) node[right] {\tiny Accuracy};
\draw[->] (0,0) -- (0,4) node[above] {\tiny Interpretability};

% Models plotted
\node[draw, circle, fill=forest!40] (lr) at (1,3.5) {\tiny LR};
\node[draw, circle, fill=navyblue!40] (dt) at (2,3) {\tiny DT};
\node[draw, circle, fill=purple!40] (rf) at (3.5,1.5) {\tiny RF};
\node[draw, circle, fill=crimson!40] (nn) at (5,0.5) {\tiny NN};

% Dashed tradeoff line
\draw[dashed, thick] (0.5,4) -- (5.5,0);

% Labels
\node at (1,4.2) {\tiny Interpretable};
\node at (5,4.2) {\tiny "Black Box"};
\end{tikzpicture}
\end{figure}

\textbf{Traditional View:}
\begin{itemize}
\item Simple models = interpretable but less accurate
\item Complex models = accurate but black box
\end{itemize}

\vspace{0.2cm}

\textbf{Modern View:}
\begin{itemize}
\item Use complex models for accuracy
\item Add interpretability methods post-hoc
\item Best of both worlds!
\end{itemize}
\end{column}

\begin{column}{0.5\textwidth}
\textbf{Why We Need Interpretability:}

\begin{enumerate}
\item \textcolor{forest}{\textbf{Trust \& Adoption}}
   \begin{itemize}
   \item Doctors won't use unexplainable diagnosis
   \item Banks must explain loan decisions
   \end{itemize}

\item \textcolor{navyblue}{\textbf{Debugging \& Improvement}}
   \begin{itemize}
   \item Identify when model fails
   \item Find data leakage
   \item Improve features
   \end{itemize}

\item \textcolor{crimson}{\textbf{Regulatory Compliance}}
   \begin{itemize}
   \item GDPR "right to explanation"
   \item Fair lending laws
   \item Medical device approval
   \end{itemize}

\item \textcolor{purple}{\textbf{Fairness \& Ethics}}
   \begin{itemize}
   \item Detect bias
   \item Ensure equity
   \item Build responsible AI
   \end{itemize}

\item \textcolor{gold}{\textbf{Scientific Discovery}}
   \begin{itemize}
   \item Understand relationships
   \item Generate hypotheses
   \item Learn from model
   \end{itemize}
\end{enumerate}
\end{column}
\end{columns}

\vspace{0.2cm}

\begin{alertblock}{Real-World Example}
COMPAS recidivism algorithm: Used to predict re-offense. Found to be biased against Black defendants. Interpretability revealed the problem, led to reform.
\end{alertblock}
\end{frame}

\begin{frame}[fragile]{Types of Interpretability}
\textbf{Taxonomy of explanation methods}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Intrinsic vs. Post-hoc:}

\begin{table}
\tiny
\begin{tabular}{|p{2.3cm}|p{2.3cm}|}
\hline
\rowcolor{navyblue!20}
\textbf{Intrinsic} & \textbf{Post-hoc} \\
\hline
Model is inherently interpretable & Apply to any model after training \\
\hline
Linear regression, decision trees & SHAP, LIME \\
\hline
Limited complexity & Can explain black boxes \\
\hline
\end{tabular}
\end{table}

\vspace{0.3cm}

\textbf{Model-Specific vs. Model-Agnostic:}

\begin{table}
\tiny
\begin{tabular}{|p{2.3cm}|p{2.3cm}|}
\hline
\rowcolor{navyblue!20}
\textbf{Model-Specific} & \textbf{Model-Agnostic} \\
\hline
Uses model internals & Treats model as black box \\
\hline
Attention weights, neuron activations & LIME, PDP \\
\hline
More accurate & Works for any model \\
\hline
\end{tabular}
\end{table}

\vspace{0.3cm}

\textbf{Global vs. Local:}

\begin{itemize}
\item \textbf{Global:} How model works overall
  \begin{itemize}
  \item Feature importance
  \item Partial dependence plots
  \end{itemize}

\item \textbf{Local:} Why this prediction?
  \begin{itemize}
  \item LIME, SHAP values
  \item Counterfactual explanations
  \end{itemize}
\end{itemize}
\end{column}

\begin{column}{0.5\textwidth}
\textbf{Levels of Interpretability:}

\begin{enumerate}
\item \textcolor{forest}{\textbf{Algorithm Transparency}}
   \begin{itemize}
   \item How does the algorithm work?
   \item Mathematical properties
   \end{itemize}

\item \textcolor{navyblue}{\textbf{Global Model Interpretability}}
   \begin{itemize}
   \item What did the model learn overall?
   \item Feature importances
   \item Decision rules
   \end{itemize}

\item \textcolor{crimson}{\textbf{Local Interpretability}}
   \begin{itemize}
   \item Why this specific prediction?
   \item Which features mattered for this instance?
   \end{itemize}
\end{enumerate}

\vspace{0.3cm}

\textbf{The Interpretability Toolbox:}

\begin{table}
\tiny
\begin{tabular}{|l|l|l|}
\hline
\rowcolor{navyblue!20}
\textbf{Method} & \textbf{Scope} & \textbf{Type} \\
\hline
Feature Importance & Global & Model-specific \\
\hline
Partial Dependence & Global & Model-agnostic \\
\hline
SHAP & Both & Model-agnostic \\
\hline
LIME & Local & Model-agnostic \\
\hline
Counterfactuals & Local & Model-agnostic \\
\hline
Attention Weights & Local & Model-specific (NN) \\
\hline
\end{tabular}
\end{table}
\end{column}
\end{columns}
\end{frame}

% ================================================================
% PART 2: MODEL-SPECIFIC INTERPRETABILITY
% ================================================================

\begin{frame}[fragile]{Intrinsically Interpretable Models}
\textbf{Models we can directly understand}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Linear Regression:}

\[
\hat{y} = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p
\]

\textbf{Interpretation:}
\begin{itemize}
\item $\beta_j$: Change in $y$ for 1-unit change in $x_j$
\item Sign: positive/negative relationship
\item Magnitude: strength of effect
\end{itemize}

\textbf{Caveats:}
\begin{itemize}
\item Assumes linearity
\item Correlation $\neq$ causation
\item Multicollinearity issues
\end{itemize}

\vspace{0.3cm}

\textbf{Logistic Regression:}

\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + \cdots
\]

\textbf{Interpretation:}
\begin{itemize}
\item $\exp(\beta_j)$: Odds ratio
\item $\beta_j > 0$: Increases probability
\end{itemize}
\end{column}

\begin{column}{0.5\textwidth}
\textbf{Decision Trees:}

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.6]
% Root
\node[draw, rectangle, fill=navyblue!20] (root) at (3,4) {Age $\leq 30$?};

% Level 1
\node[draw, rectangle, fill=forest!20] (l1) at (1,2.5) {Income $\leq$ 50K?};
\node[draw, rectangle, fill=forest!20] (r1) at (5,2.5) {Credit $\geq$ 700?};

% Leaves
\node[draw, circle, fill=crimson!20] (ll) at (0,1) {No};
\node[draw, circle, fill=green!40] (lr) at (2,1) {Yes};
\node[draw, circle, fill=green!40] (rl) at (4,1) {Yes};
\node[draw, circle, fill=crimson!20] (rr) at (6,1) {No};

% Edges
\draw[->, thick] (root) -- node[left] {\tiny Yes} (l1);
\draw[->, thick] (root) -- node[right] {\tiny No} (r1);
\draw[->, thick] (l1) -- node[left] {\tiny Y} (ll);
\draw[->, thick] (l1) -- node[right] {\tiny N} (lr);
\draw[->, thick] (r1) -- node[left] {\tiny Y} (rl);
\draw[->, thick] (r1) -- node[right] {\tiny N} (rr);
\end{tikzpicture}
\end{figure}

\textbf{Interpretation:}
\begin{itemize}
\item Clear if-then rules
\item Easy to visualize
\item Mimics human decision-making
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
\item High variance (unstable)
\item Can be very deep (not interpretable)
\end{itemize}

\vspace{0.2cm}

\textbf{Rule-Based Models:}

Extract rules from tree ensembles:
\begin{verbatim}
IF age > 30 AND income > 50K
   THEN approve_loan (95% confidence)
\end{verbatim}
\end{column}
\end{columns}
\end{frame}

% ================================================================
% PART 3: SHAP (SHapley Additive exPlanations)
% ================================================================

\begin{frame}[fragile]{SHAP: Unified Framework for Interpretability}
\textbf{Game theory meets machine learning}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{The Core Idea:}

How much does each feature contribute to prediction?

\textbf{Based on Shapley Values from Game Theory:}

Consider prediction as "coalition game":
\begin{itemize}
\item Players = features
\item Payoff = prediction
\item Fair distribution of credit
\end{itemize}

\vspace{0.2cm}

\textbf{SHAP Value for Feature $i$:}

\[
\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} [f(S \cup \{i\}) - f(S)]
\]

\textbf{Intuition:}
\begin{itemize}
\item Average marginal contribution
\item Across all possible feature subsets
\item Accounts for feature interactions
\end{itemize}

\vspace{0.2cm}

\textbf{Properties:}
\begin{enumerate}
\item \textbf{Local accuracy:} Sum equals prediction
\item \textbf{Consistency:} More contribution $\to$ higher value
\item \textbf{Missingness:} Missing features have zero impact
\end{enumerate}
\end{column}

\begin{column}{0.5\textwidth}
\textbf{SHAP in Practice:}

\begin{verbatim}
import shap

# Train model
model = XGBClassifier()
model.fit(X_train, y_train)

# Create explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test)
shap.force_plot(explainer.expected_value,
                shap_values[0], X_test.iloc[0])
\end{verbatim}

\vspace{0.2cm}

\textbf{SHAP Visualizations:}

\begin{enumerate}
\item \textbf{Force plot:} Individual prediction
  \begin{itemize}
  \item Base value + feature contributions
  \end{itemize}

\item \textbf{Summary plot:} Global feature importance
  \begin{itemize}
  \item Feature impact across all instances
  \end{itemize}

\item \textbf{Dependence plot:} Feature effects
  \begin{itemize}
  \item How feature value affects prediction
  \item Shows interactions
  \end{itemize}

\item \textbf{Waterfall plot:} Step-by-step explanation
\end{enumerate}
\end{column}
\end{columns}

\vspace{0.2cm}

\begin{alertblock}{Key Advantage}
SHAP provides theoretically sound, consistent explanations for ANY model!
\end{alertblock}
\end{frame}

\begin{frame}[fragile]{LIME: Local Interpretable Model-Agnostic Explanations}
\textbf{Explain any model locally with simple approximations}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{The LIME Approach:}

\textbf{Key Idea:} Approximate complex model locally with interpretable model

\vspace{0.2cm}

\textbf{Algorithm:}
\begin{enumerate}
\item Select instance to explain
\item Generate perturbed samples nearby
\item Get predictions from black box
\item Train simple model (linear/tree) on perturbed data
\item Use simple model to explain
\end{enumerate}

\vspace{0.3cm}

\textbf{Mathematical Formulation:}

\[
\xi(x) = \argmin_{g \in \mathcal{G}} \mathcal{L}(f, g, \pi_x) + \Omega(g)
\]

where:
\begin{itemize}
\item $f$: Black box model
\item $g$: Simple interpretable model
\item $\pi_x$: Proximity measure
\item $\Omega(g)$: Complexity penalty
\end{itemize}
\end{column}

\begin{column}{0.5\textwidth}
\textbf{LIME Implementation:}

\begin{verbatim}
from lime import lime_tabular

# Create explainer
explainer = lime_tabular.
    LimeTabularExplainer(
        X_train,
        feature_names=feature_names,
        class_names=['No', 'Yes'],
        mode='classification'
    )

# Explain instance
explanation = explainer.explain_instance(
    X_test[0],
    model.predict_proba,
    num_features=10
)

explanation.show_in_notebook()
\end{verbatim}

\vspace{0.2cm}

\textbf{LIME vs. SHAP:}

\begin{table}
\tiny
\begin{tabular}{|l|p{4cm}|}
\hline
\rowcolor{navyblue!20}
\textbf{Aspect} & \textbf{Comparison} \\
\hline
Theory & SHAP: game theory, LIME: local approx \\
\hline
Consistency & SHAP: guaranteed, LIME: not always \\
\hline
Speed & LIME: faster, SHAP: can be slow \\
\hline
Images/Text & Both support well \\
\hline
\end{tabular}
\end{table}
\end{column}
\end{columns}
\end{frame}

% Due to length, including key additional topics in summary form

\begin{frame}[fragile]{Feature Importance Methods}
\textbf{Understanding what the model learned globally}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{1. Permutation Importance:}

Shuffle feature $j$, measure performance drop:

\begin{verbatim}
from sklearn.inspection import
    permutation_importance

result = permutation_importance(
    model, X_test, y_test,
    n_repeats=10
)
\end{verbatim}

\textbf{Advantages:}
\begin{itemize}
\item Model-agnostic
\item Reliable
\item Accounts for interactions
\end{itemize}

\vspace{0.2cm}

\textbf{2. Drop-Column Importance:}

Retrain without feature, measure drop:
\begin{itemize}
\item More accurate but expensive
\item Shows feature necessity
\end{itemize}

\vspace{0.2cm}

\textbf{3. Tree-Based Importance:}

For random forests, XGBoost:
\begin{itemize}
\item Gini importance
\item Split counts
\item Fast but can be biased
\end{itemize}
\end{column}

\begin{column}{0.5\textwidth}
\textbf{4. Partial Dependence Plots (PDP):}

Show marginal effect of feature:

\[
\hat{f}_S(x_S) = \E_{x_C}[\hat{f}(x_S, x_C)]
\]

\begin{verbatim}
from sklearn.inspection import
    PartialDependenceDisplay

PartialDependenceDisplay.from_estimator(
    model, X, features=[0, 1, (0,1)]
)
\end{verbatim}

\textbf{Shows:}
\begin{itemize}
\item Feature effect on average
\item Non-linear relationships
\item 2D: Feature interactions
\end{itemize}

\vspace{0.2cm}

\textbf{5. Individual Conditional Expectation (ICE):}

PDP for each instance:
\begin{itemize}
\item Shows heterogeneity
\item Reveals subgroups
\item More detailed than PDP
\end{itemize}
\end{column}
\end{columns}

\vspace{0.2cm}

\begin{alertblock}{Best Practice}
Use multiple methods! Permutation + SHAP + PDP gives complete picture.
\end{alertblock}
\end{frame}

\begin{frame}[fragile]{Counterfactual Explanations}
\textbf{"What would need to change for a different outcome?"}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{The Counterfactual Question:}

"Why was my loan denied?"

\textbf{Bad Answer:}
"Credit score was too low (SHAP value = -0.3)"

\textbf{Better Answer (Counterfactual):}
"If your credit score were 680 instead of 620, you would be approved"

\vspace{0.3cm}

\textbf{Properties of Good Counterfactuals:}

\begin{enumerate}
\item \textcolor{forest}{\textbf{Proximity:}} Close to original instance
\item \textcolor{navyblue}{\textbf{Sparsity:}} Change few features
\item \textcolor{crimson}{\textbf{Actionability:}} Can actually change
\item \textcolor{purple}{\textbf{Plausibility:}} Realistic values
\end{enumerate}

\vspace{0.2cm}

\textbf{Mathematical Formulation:}

\[
\argmin_{x'} d(x, x') + \lambda \cdot \mathbb{1}[f(x') \neq y']
\]

Find nearest instance with different prediction.
\end{column}

\begin{column}{0.5\textwidth}
\textbf{Methods:}

\begin{enumerate}
\item \textbf{Optimization-Based:}
   \begin{itemize}
   \item Minimize distance subject to prediction constraint
   \item Can use genetic algorithms
   \end{itemize}

\item \textbf{DiCE} (Diverse Counterfactual Explanations):
   \begin{itemize}
   \item Generate multiple diverse counterfactuals
   \item Gives user options
   \end{itemize}
\end{enumerate}

\vspace{0.2cm}

\textbf{Implementation:}

\begin{verbatim}
import dice_ml

# Create DiCE explainer
dice = dice_ml.Dice(data, model)

# Generate counterfactuals
cf = dice.generate_counterfactuals(
    instance,
    total_CFs=3,
    desired_class="opposite"
)
cf.visualize_as_dataframe()
\end{verbatim}

\vspace{0.2cm}

\textbf{Use Cases:}
\begin{itemize}
\item Loan applications
\item College admissions
\item Medical diagnosis
\item Hiring decisions
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[fragile]{Knowledge Check: Explainable AI}
\textbf{Test your understanding}

\vspace{0.2cm}

\textbf{Question 1:} What is the key difference between SHAP and LIME?

\begin{enumerate}[A)]
\item SHAP is for trees only, LIME is model-agnostic
\item \textcolor{forest}{\textbf{SHAP has theoretical guarantees (Shapley values), LIME uses local approximation}}
\item LIME is faster
\item They are the same
\end{enumerate}

\pause

\begin{block}{Explanation}
\textcolor{forest}{\textbf{Answer: B}} - SHAP is based on game-theoretic Shapley values with mathematical guarantees (local accuracy, consistency). LIME approximates the model locally with a simpler model, which is fast but doesn't have the same theoretical properties. Both are model-agnostic, but SHAP provides more consistent explanations.
\end{block}

\pause

\vspace{0.2cm}

\textbf{Question 2:} When would you use counterfactual explanations over feature importance?

\begin{enumerate}[A)]
\item When you need global model understanding
\item \textcolor{forest}{\textbf{When users need actionable advice on how to change outcomes}}
\item When the model is a decision tree
\item Never, feature importance is always better
\end{enumerate}

\pause

\begin{block}{Explanation}
\textcolor{forest}{\textbf{Answer: B}} - Counterfactuals answer "what would I need to change?" which is actionable. Example: "Increase income by \$10K to get approved" vs. "Income has high feature importance" (not actionable). Perfect for loan applications, college admissions, medical interventions where people want to know how to achieve different outcomes.
\end{block}
\end{frame}

\begin{frame}[fragile]{Summary \& Best Practices}
\textbf{Building interpretable ML systems}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{The Interpretability Workflow:}

\begin{enumerate}
\item \textbf{Model Selection:}
   \begin{itemize}
   \item Start with interpretable baselines
   \item Document accuracy-interpretability tradeoff
   \item Justify complexity if needed
   \end{itemize}

\item \textbf{Global Understanding:}
   \begin{itemize}
   \item Permutation importance
   \item Partial dependence plots
   \item SHAP summary plots
   \end{itemize}

\item \textbf{Local Explanations:}
   \begin{itemize}
   \item SHAP force plots for key predictions
   \item LIME for individual cases
   \item Counterfactuals for recourse
   \end{itemize}

\item \textbf{Validation:}
   \begin{itemize}
   \item Check explanations make sense
   \item Verify with domain experts
   \item Test on edge cases
   \end{itemize}

\item \textbf{Communication:}
   \begin{itemize}
   \item Tailor to audience
   \item Visualize clearly
   \item Provide actionable insights
   \end{itemize}
\end{enumerate}
\end{column}

\begin{column}{0.5\textwidth}
\textbf{Common Mistakes:}

\begin{itemize}
\item[$\times$] Trusting explanations blindly
\item[$\times$] Using only one method
\item[$\times$] Ignoring feature correlations
\item[$\times$] Not validating with experts
\item[$\times$] Over-interpreting complex models
\end{itemize}

\vspace{0.3cm}

\textbf{Tools \& Resources:}

\begin{itemize}
\item \textbf{Python Libraries:}
  \begin{itemize}
  \item SHAP, LIME, ELI5
  \item InterpretML (Microsoft)
  \item AIX360 (IBM)
  \item Captum (PyTorch)
  \end{itemize}

\item \textbf{Books:}
  \begin{itemize}
  \item "Interpretable Machine Learning" (Molnar)
  \item "Explanatory Model Analysis" (Biecek \& Burzykowski)
  \end{itemize}

\item \textbf{Papers:}
  \begin{itemize}
  \item "A Unified Approach to Interpreting Model Predictions" (SHAP)
  \item "Why Should I Trust You?" (LIME)
  \end{itemize}
\end{itemize}

\vspace{0.2cm}

\begin{alertblock}{Remember}
Interpretability is not optional - it's essential for trustworthy AI!
\end{alertblock}
\end{column}
\end{columns}
\end{frame}

% ================================================================
% END OF EXPLAINABLE AI PRESENTATION
% ================================================================
