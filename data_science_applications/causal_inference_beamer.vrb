\frametitle{Case Study: Online Marketing Campaign Evaluation}
\textbf{Business Problem:} Did our email marketing campaign increase sales?

\begin{columns}
\begin{column}{0.55\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\tiny]
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn as sns

# Simulate marketing campaign data
np.random.seed(42)
n_customers = 5000

# Customer characteristics
age = np.random.normal(40, 15, n_customers)
income = np.random.lognormal(10, 0.5, n_customers)
previous_purchases = np.random.poisson(3, n_customers)
email_engagement = np.random.beta(2, 5, n_customers)

# Selection bias: more engaged customers more likely to receive campaign
campaign_prob = 0.3 + 0.4 * email_engagement
received_campaign = np.random.binomial(1, campaign_prob, n_customers)

# Outcome: purchase in next month
# Confounded by engagement
base_purchase_prob = (0.1 + 0.2 * email_engagement +
                     0.1 * (previous_purchases > 2) +
                     0.05 * (income > np.median(income)))

# True campaign effect: 8 percentage points
campaign_effect = 0.08
purchase_prob = (base_purchase_prob +
                campaign_effect * received_campaign)
purchased = np.random.binomial(1, purchase_prob, n_customers)

df = pd.DataFrame({
    'age': age,
    'income': income,
    'previous_purchases': previous_purchases,
    'email_engagement': email_engagement,
    'received_campaign': received_campaign,
    'purchased': purchased
})

print(f"True campaign effect: {campaign_effect:.1%}")

# Naive comparison (biased due to selection)
naive_effect = (df[df['received_campaign']==1]['purchased'].mean() -
                df[df['received_campaign']==0]['purchased'].mean())
print(f"Naive estimate: {naive_effect:.1%}")

# Propensity score matching
# Step 1: Estimate propensity scores
X_ps = df[['age', 'income', 'previous_purchases', 'email_engagement']]
X_ps_scaled = StandardScaler().fit_transform(X_ps)

ps_model = LogisticRegression(random_state=42)
ps_model.fit(X_ps_scaled, df['received_campaign'])
propensity_scores = ps_model.predict_proba(X_ps_scaled)[:, 1]

df['propensity_score'] = propensity_scores

# Step 2: Check overlap
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.hist(df[df['received_campaign']==0]['propensity_score'],
         alpha=0.7, label='Control', bins=30)
plt.hist(df[df['received_campaign']==1]['propensity_score'],
         alpha=0.7, label='Treatment', bins=30)
plt.xlabel('Propensity Score')
plt.ylabel('Frequency')
plt.title('Propensity Score Distribution')
plt.legend()

# Step 3: Simple propensity score weighting (IPW)
# Weight = 1/P(T|X) for treated, 1/(1-P(T|X)) for control
weights = np.where(df['received_campaign'] == 1,
                  1 / df['propensity_score'],
                  1 / (1 - df['propensity_score']))

# Trim extreme weights
weights = np.clip(weights, 0, np.percentile(weights, 95))
df['weights'] = weights

# Weighted average treatment effect
weighted_treated = np.average(df[df['received_campaign']==1]['purchased'],
                             weights=df[df['received_campaign']==1]['weights'])
weighted_control = np.average(df[df['received_campaign']==0]['purchased'],
                             weights=df[df['received_campaign']==0]['weights'])
ipw_effect = weighted_treated - weighted_control

print(f"IPW estimate: {ipw_effect:.1%}")

# Step 4: Regression adjustment
from sklearn.ensemble import RandomForestClassifier

# Doubly robust estimation
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
X_features = ['age', 'income', 'previous_purchases', 'email_engagement', 'propensity_score']
rf_model.fit(df[X_features], df['purchased'])

# Predict potential outcomes
df_treated = df.copy()
df_treated['received_campaign'] = 1
df_control = df.copy()
df_control['received_campaign'] = 0

# Note: This is a simplified DR approach
mu1_hat = rf_model.predict_proba(df_treated[X_features])[:, 1]
mu0_hat = rf_model.predict_proba(df_control[X_features])[:, 1]

dr_effect = np.mean(mu1_hat - mu0_hat)
print(f"Doubly robust estimate: {dr_effect:.1%}")

# Visualize balance before/after weighting
plt.subplot(1, 3, 2)
unweighted_diff = abs(df[df['received_campaign']==1]['email_engagement'].mean() -
                     df[df['received_campaign']==0]['email_engagement'].mean())
weighted_diff = abs(np.average(df[df['received_campaign']==1]['email_engagement'],
                              weights=df[df['received_campaign']==1]['weights']) -
                   np.average(df[df['received_campaign']==0]['email_engagement'],
                              weights=df[df['received_campaign']==0]['weights']))

plt.bar(['Before', 'After'], [unweighted_diff, weighted_diff])
plt.ylabel('Standardized Difference')
plt.title('Covariate Balance: Email Engagement')

plt.subplot(1, 3, 3)
methods = ['Naive', 'IPW', 'Doubly Robust', 'True']
estimates = [naive_effect, ipw_effect, dr_effect, campaign_effect]
colors = ['red', 'orange', 'green', 'blue']

plt.bar(methods, estimates, color=colors, alpha=0.7)
plt.ylabel('Treatment Effect')
plt.title('Method Comparison')
plt.axhline(campaign_effect, color='blue', linestyle='--', alpha=0.7)

plt.tight_layout()
print("Marketing campaign analysis complete")
\end{lstlisting}
\end{column}
\begin{column}{0.45\textwidth}
\textbf{Identification Strategy:}
\begin{enumerate}
\item \textbf{Problem}: Selection bias - engaged customers more likely to receive campaign
\item \textbf{Solution}: Propensity score methods
\item \textbf{Assumption}: No unobserved confounders (selection on observables)
\end{enumerate}

\vspace{0.3cm}
\textbf{Methods Applied:}
\begin{itemize}
\item \textbf{Naive comparison}: Biased upward
\item \textbf{Inverse propensity weighting}: Reweight to balance groups
\item \textbf{Doubly robust}: Combine propensity scores with outcome modeling
\end{itemize}

\vspace{0.3cm}
\textbf{Key Diagnostics:}
\begin{itemize}
\item \textcolor{forest}{Overlap}: Sufficient common support
\item \textcolor{forest}{Balance}: Covariates similar after weighting
\item \textcolor{forest}{Sensitivity}: How robust to unobserved confounding?
\end{itemize}

\vspace{0.3cm}
\begin{block}{Business Insights}
\begin{itemize}
\item Campaign had positive effect (~8pp)
\item Selection bias inflated naive estimate
\item Target similar customers in future
\item Consider randomized campaigns for better identification
\end{itemize}
\end{block}
\end{column}
\end{columns}
