\documentclass{beamer}

% Theme and appearance
\usetheme{Madrid}
\usecolortheme{default}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}

% Title info
\title[Stationarity \& Ergodicity]{Stationarity and Ergodicity in Time Series}
\subtitle{From Intuition to Theory}
\author{Diogo Ribeiro}
\institute{%
  \textit{Data Science and Applied Mathematics}
}
\date{\today}

% Optional: reduce clutter in navigation
\setbeamertemplate{navigation symbols}{}

\begin{document}

%==================================================
% Title + Outline
%==================================================

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%==================================================
\section{Motivation and Examples}
%==================================================

\begin{frame}{Why Time Series?}
  \begin{itemize}
    \item Many real-world phenomena are naturally ordered in time:
      \begin{itemize}
        \item Financial returns, volatility, prices.
        \item Temperature and climate indicators.
        \item Sensor data, biomedical signals, internet traffic.
      \end{itemize}
    \item We often observe \textbf{one long history}, not repeated experiments.
    \item Aim:
      \begin{itemize}
        \item Understand the probabilistic mechanism generating the series.
        \item Forecast future values.
        \item Detect changes, regimes, or anomalies.
      \end{itemize}
  \end{itemize}

  \vspace{0.3cm}
  Stationarity and ergodicity tell us when the past is informative about the future and about the underlying process.
\end{frame}

\begin{frame}{Typical Time Series Questions}
  \begin{itemize}
    \item Is the mean level of the series stable over time?
    \item Does the variability change (volatility clustering, heteroscedasticity)?
    \item Are there trends or seasonal patterns?
    \item Can we treat one observed trajectory as \emph{representative} of the process?
  \end{itemize}

  \vspace{0.3cm}
  \textbf{Stationarity} addresses stability of distributions over time.  
  \textbf{Ergodicity} connects time averages and ensemble averages.
\end{frame}

%==================================================
\section{Stochastic Processes and Perspectives}
%==================================================

\begin{frame}{Stochastic Process: Interaction with Time}
  \begin{itemize}
    \item A time series is a realization of a stochastic process:
      \[
        \{X_t\}_{t \in \mathbb{Z}} \quad \text{or} \quad \{X_t\}_{t \in \mathbb{R}}.
      \]
    \item For each time $t$, $X_t$ is a random variable defined on some probability space.
    \item A realization (sample path) is one function $t \mapsto x_t$ generated by the process.
    \item In practice we observe:
      \[
        x_1, x_2, \dots, x_T.
      \]
  \end{itemize}

  The challenge: infer properties of the \emph{process} from a single time-ordered sample.
\end{frame}

\begin{frame}{Two Perspectives: Time and Ensemble}
  \begin{itemize}
    \item \textbf{Time perspective} (within one realization):
      \begin{itemize}
        \item Follow $X_t$ for $t=1,\dots,T$.
        \item Compute time averages, such as sample mean and sample variance.
      \end{itemize}
    \item \textbf{Ensemble perspective} (across realizations):
      \begin{itemize}
        \item At fixed time $t$, consider many copies $X_t^{(1)}, \dots, X_t^{(N)}$.
        \item The distribution of these is the \emph{ensemble distribution} at time $t$.
      \end{itemize}
    \item In practice, we almost never have many independent realizations from the same process.
  \end{itemize}

  \vspace{0.3cm}
  This tension between time and ensemble is where stationarity and ergodicity enter.
\end{frame}

\begin{frame}{Fundamental Problem}
  \begin{itemize}
    \item We want ensemble quantities, such as $E[X_t]$, $\mathrm{Var}(X_t)$, or joint distributions.
    \item We only have a single path:
      \[
        x_1, x_2, \dots, x_T.
      \]
    \item Natural estimators:
      \[
        \bar{X}_T = \frac{1}{T} \sum_{t=1}^T X_t,
        \quad
        s_T^2 = \frac{1}{T-1} \sum_{t=1}^T (X_t - \bar{X}_T)^2.
      \]
    \item When is $\bar{X}_T$ a good estimator of $E[X_t]$?  
          When does $s_T^2$ estimate $\mathrm{Var}(X_t)$?
  \end{itemize}

  \vspace{0.2cm}
  Answering this rigorously requires both stationarity and ergodicity.
\end{frame}

%==================================================
\section{Stationarity}
%==================================================

\begin{frame}{Strict (Strong) Stationarity}
  \textbf{Definition.}  
  A process $\{X_t\}$ is \emph{strictly stationary} if for any $k \in \mathbb{N}$, any times
  \[
    t_1, \dots, t_k
  \]
  and any integer shift $h$, the joint distributions satisfy
  \[
    (X_{t_1}, \dots, X_{t_k}) \overset{d}{=} (X_{t_1+h}, \dots, X_{t_k+h}).
  \]

  \vspace{0.3cm}
  Intuition:
  \begin{itemize}
    \item The probabilistic structure of the process is invariant under time shifts.
    \item All finite-dimensional distributions are time-homogeneous.
  \end{itemize}
\end{frame}

\begin{frame}{Weak (Covariance) Stationarity}
  \textbf{Definition.}  
  A process $\{X_t\}$ is \emph{weakly stationary} (or covariance stationary) if:
  \begin{enumerate}
    \item $E[X_t] = \mu$ is constant for all $t$.
    \item $\mathrm{Var}(X_t) = \sigma^2 < \infty$ is constant for all $t$.
    \item $\mathrm{Cov}(X_t, X_{t+h})$ depends only on $h$, not on $t$:
      \[
        \gamma(h) = \mathrm{Cov}(X_t, X_{t+h}).
      \]
  \end{enumerate}

  \vspace{0.3cm}
  This is enough for many linear time series models (ARMA, etc.) and for spectral analysis.
\end{frame}

\begin{frame}{Autocovariance and Autocorrelation}
  For a weakly stationary process:
  \begin{itemize}
    \item Autocovariance function (ACVF):
      \[
        \gamma(h) = \mathrm{Cov}(X_t, X_{t+h}), \quad h \in \mathbb{Z}.
      \]
    \item Autocorrelation function (ACF):
      \[
        \rho(h) = \frac{\gamma(h)}{\gamma(0)}.
      \]
  \end{itemize}

  \vspace{0.3cm}
  Properties:
  \begin{itemize}
    \item $\gamma(0) = \sigma^2$.
    \item $\gamma(-h) = \gamma(h)$.
    \item $\rho(0) = 1$, and $|\rho(h)| \le 1$ for all $h$.
  \end{itemize}
\end{frame}

\begin{frame}{Examples of Stationary Processes}
  \textbf{Example 1: White noise}
  \begin{itemize}
    \item $X_t \sim \text{i.i.d.}(0, \sigma^2)$.
    \item $E[X_t] = 0$, $\gamma(0) = \sigma^2$, $\gamma(h) = 0$ for $h \neq 0$.
    \item Strictly and weakly stationary.
  \end{itemize}

  \vspace{0.3cm}
  \textbf{Example 2: AR(1) with $| \phi | < 1$}
  \begin{align*}
    X_t &= \phi X_{t-1} + \varepsilon_t, \\
    \varepsilon_t &\sim \text{i.i.d.}(0, \sigma_\varepsilon^2), \quad |\phi|<1.
  \end{align*}
  Then:
  \[
    E[X_t] = 0, \quad
    \gamma(h) = \frac{\sigma_\varepsilon^2}{1 - \phi^2} \phi^{|h|}.
  \]
  This process is weakly stationary and, under mild conditions, strictly stationary.
\end{frame}

\begin{frame}{Non-stationary Example: Random Walk}
  \textbf{Random walk:}
  \begin{align*}
    X_t &= X_{t-1} + \varepsilon_t, \\
    \varepsilon_t &\sim \text{i.i.d.}(0, \sigma_\varepsilon^2),
    \quad X_0 \text{ given}.
  \end{align*}

  \vspace{0.2cm}
  \begin{itemize}
    \item $E[X_t] = E[X_0]$ (constant mean).
    \item $\mathrm{Var}(X_t) = \mathrm{Var}(X_0) + t \sigma_\varepsilon^2$ grows with $t$.
    \item The variance is not constant \textrightarrow{} not even weakly stationary.
  \end{itemize}

  \vspace{0.2cm}
  Differencing $\Delta X_t = X_t - X_{t-1} = \varepsilon_t$ recovers a stationary series (white noise).
\end{frame}

\begin{frame}{Strict vs Weak Stationarity}
  \begin{itemize}
    \item Strict stationarity $\Rightarrow$ weak stationarity if second moments exist.
    \item Weak stationarity does not necessarily imply strict stationarity.
    \item In practice:
      \begin{itemize}
        \item We rarely can test strict stationarity.
        \item Most modelling frameworks assume weak stationarity.
      \end{itemize}
  \end{itemize}

  \vspace{0.3cm}
  For many Gaussian processes, weak stationarity actually implies strict stationarity, because the distribution is fully determined by mean and covariance.
\end{frame}

%==================================================
\section{Ergodicity}
%==================================================

\begin{frame}{Time Averages vs Ensemble Averages}
  For a weakly stationary process $\{X_t\}$ with finite mean $\mu$:
  \begin{itemize}
    \item Ensemble mean:
      \[
        \mu = E[X_t].
      \]
    \item Time average (sample mean) over one trajectory:
      \[
        \bar{X}_T = \frac{1}{T}\sum_{t=1}^T X_t.
      \]
  \end{itemize}

  \vspace{0.3cm}
  \textbf{Question:} Under what conditions does
  \[
    \bar{X}_T \xrightarrow[T \to \infty]{\text{(some sense)}} \mu?
  \]
  This is the core idea behind \emph{ergodicity}.
\end{frame}

\begin{frame}{Ergodicity in the Mean}
  \textbf{Definition (informal).}  
  A stationary process $\{X_t\}$ is \emph{ergodic in the mean} if
  \[
    \bar{X}_T = \frac{1}{T}\sum_{t=1}^T X_t
    \xrightarrow[T \to \infty]{\text{a.s. or in prob.}}
    E[X_t] = \mu.
  \]

  \vspace{0.3cm}
  Interpretation:
  \begin{itemize}
    \item A single long realization is enough to estimate the mean.
    \item The time average along one trajectory converges to the ensemble expectation.
  \end{itemize}
\end{frame}

\begin{frame}{Ergodicity for Higher Moments}
  We can extend the notion of ergodicity:
  \begin{itemize}
    \item \textbf{Ergodic in variance:}
      \[
        \frac{1}{T}\sum_{t=1}^T (X_t - \bar{X}_T)^2
        \xrightarrow[T \to \infty]{}
        \mathrm{Var}(X_t).
      \]
    \item \textbf{Ergodic for autocovariances:}
      \[
        \frac{1}{T}\sum_{t=1}^{T-h} X_t X_{t+h}
        \xrightarrow[T \to \infty]{}
        \gamma(h).
      \]
  \end{itemize}

  \vspace{0.3cm}
  In practice, we estimate $\gamma(h)$ from one series and implicitly assume ergodicity.
\end{frame}

\begin{frame}{Ergodic Theorem (Very Informal)}
  \textbf{Birkhoff's Ergodic Theorem (informal statement):}
  \begin{itemize}
    \item Consider a measure-preserving transformation $T$ on a probability space and an integrable function $f$.
    \item Under ergodicity of $T$, the time average along orbits converges (almost surely) to the space average:
      \[
        \frac{1}{n}\sum_{k=0}^{n-1} f(T^k \omega)
        \xrightarrow[n\to\infty]{\text{a.s.}}
        \int f \, d\mathbb{P}.
      \]
  \end{itemize}

  \vspace{0.3cm}
  Time averages are legitimate estimators of expectations if the underlying dynamical system is ergodic.
\end{frame}

\begin{frame}{Stationarity vs Ergodicity}
  \begin{itemize}
    \item Stationarity:
      \begin{itemize}
        \item Distributions do not change with time shifts.
        \item Structural property of the process.
      \end{itemize}
    \item Ergodicity:
      \begin{itemize}
        \item Time averages equal ensemble averages.
        \item Relates individual realizations to the overall distribution.
      \end{itemize}
  \end{itemize}

  \vspace{0.3cm}
  Relationship:
  \begin{itemize}
    \item Ergodicity \emph{implies} stationarity (in appropriate formulations).
    \item Stationarity alone does not guarantee ergodicity.
  \end{itemize}
\end{frame}

\begin{frame}{Examples: When Is a Process Ergodic?}
  \begin{itemize}
    \item Many ARMA processes with $|\phi_i|<1$ and i.i.d.\ innovations are stationary and ergodic.
    \item Gaussian stationary processes often satisfy ergodic properties under mild conditions.
    \item A process that is a mixture of two stationary regimes but never switches between them can be stationary and non-ergodic.
  \end{itemize}

  \vspace{0.3cm}
  Informally, ergodicity fails when different realizations live in different ``parts'' of the state space and never explore the whole distribution.
\end{frame}

%==================================================
\section{Practical Diagnostics}
%==================================================

\begin{frame}{Checking Stationarity in Practice}
  \begin{itemize}
    \item \textbf{Visual inspection:}
      \begin{itemize}
        \item Plot the series, rolling mean, and rolling variance.
        \item Look for structural breaks, changing variance, trends, seasonality.
      \end{itemize}
    \item \textbf{Unit root and stationarity tests:}
      \begin{itemize}
        \item Augmented Dickey-Fuller (ADF): null = unit root (non-stationary).
        \item KPSS: null = stationary.
        \item Phillips-Perron, DF-GLS, others.
      \end{itemize}
    \item \textbf{ACF and PACF:}
      \begin{itemize}
        \item Slowly decaying ACF suggests non-stationarity.
        \item Sudden drops suggest stationarity (for ARMA-type processes).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Transformations to Achieve Stationarity}
  \begin{itemize}
    \item \textbf{Detrending:}
      \begin{itemize}
        \item Remove deterministic trends (linear or nonlinear).
        \item Work with residuals rather than raw series.
      \end{itemize}
    \item \textbf{Differencing:}
      \[
        \nabla X_t = X_t - X_{t-1},
        \quad
        \nabla^d X_t = (1 - B)^d X_t.
      \]
    \item \textbf{Variance-stabilizing transforms:}
      \begin{itemize}
        \item Log-transform for strictly positive series.
        \item Box-Cox transformations.
      \end{itemize}
    \item \textbf{Seasonal adjustment:}
      \begin{itemize}
        \item Seasonal differencing.
        \item Removing deterministic seasonal components.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Ergodicity: Practical View}
  \begin{itemize}
    \item Directly testing ergodicity is difficult and rare in applied work.
    \item Common approach:
      \begin{itemize}
        \item Assume a model (e.g.\ ARMA, GARCH).
        \item Use known theoretical conditions for ergodicity of that model.
      \end{itemize}
    \item If the model implies ergodicity, then:
      \begin{itemize}
        \item Sample mean is a consistent estimator of the true mean.
        \item Empirical ACF estimates the theoretical ACF.
      \end{itemize}
  \end{itemize}

  \vspace{0.3cm}
  It is still important to check for structural breaks, regime changes, and non-stationarities that may violate the assumptions.
\end{frame}

%==================================================
\section{Summary and Key Messages}
%==================================================

\begin{frame}{Summary}
  \begin{itemize}
    \item A time series is one realization of an underlying stochastic process.
    \item \textbf{Stationarity} is about invariance of distributions (or moments) under time shifts.
      \begin{itemize}
        \item Strict vs weak stationarity.
        \item Many models require at least weak stationarity.
      \end{itemize}
    \item \textbf{Ergodicity} links time averages to ensemble averages.
      \begin{itemize}
        \item Justifies using long-run sample averages as estimators of the true moments.
      \end{itemize}
    \item In applications:
      \begin{itemize}
        \item Always diagnose non-stationarity.
        \item Use transformations or differencing if needed.
        \item Choose models with theoretical guarantees of stationarity and ergodicity when possible.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Takeaways for Practice}
  \begin{itemize}
    \item Never blindly assume stationarity; check data and context.
    \item Think about the mechanism: can it reasonably be stable over the sample?
    \item Use appropriate tests, but interpret them with care.
    \item When modelling, ensure that estimated parameters fall in the stationary region.
    \item Remember that all inference from one time series path implicitly relies on ergodic-type arguments.
  \end{itemize}
\end{frame}

%==================================================
\section*{References}
%==================================================

\begin{frame}{Further Reading}
  \small
  \begin{itemize}
    \item Hamilton, J. D. (1994). \emph{Time Series Analysis}. Princeton University Press.
    \item Box, G. E. P., Jenkins, G. M., Reinsel, G. C., \& Ljung, G. M. (2016). 
          \emph{Time Series Analysis: Forecasting and Control}. Wiley.
    \item Shumway, R. H., \& Stoffer, D. S. (2017).
          \emph{Time Series Analysis and Its Applications}. Springer.
    \item Brockwell, P. J., \& Davis, R. A. (2016).
          \emph{Introduction to Time Series and Forecasting}. Springer.
    \item Peters, O. (2019).
          ``The Ergodicity Problem in Economics.'' \emph{Nature Physics}, 15, 1216--1221.
  \end{itemize}
\end{frame}

%==================================================
\appendix
%==================================================

\section{Appendix: Technical Notes}

\begin{frame}{Spectral View (Optional)}
  For a weakly stationary process with ACVF $\gamma(h)$, the spectral density is
  \[
    f(\lambda) = \frac{1}{2\pi} \sum_{h=-\infty}^{\infty} \gamma(h) e^{-i \lambda h},
    \quad \lambda \in [-\pi, \pi].
  \]

  \vspace{0.3cm}
  \begin{itemize}
    \item Encodes how variance is distributed over frequencies.
    \item For ARMA processes, $f(\lambda)$ has a closed-form expression in terms of the AR and MA polynomials.
  \end{itemize}

  \vspace{0.2cm}
  Stationarity is required for the spectral representation to make sense.
\end{frame}

\begin{frame}{Conditions for Stationary AR(1)}
  Consider
  \[
    X_t = \phi X_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim \text{i.i.d.}(0, \sigma_\varepsilon^2).
  \]

  \vspace{0.3cm}
  \textbf{Key facts:}
  \begin{itemize}
    \item If $|\phi| < 1$, there exists a unique strictly stationary solution:
      \[
        X_t = \sum_{j=0}^{\infty} \phi^j \varepsilon_{t-j}.
      \]
    \item Then:
      \[
        E[X_t] = 0, \quad
        \mathrm{Var}(X_t) = \frac{\sigma_\varepsilon^2}{1 - \phi^2}.
      \]
    \item If $|\phi| \ge 1$, variance is infinite or explodes, and no weakly stationary solution exists.
  \end{itemize}
\end{frame}

\begin{frame}{Thank You}
  \centering
  Questions or discussion?
\end{frame}

\end{document}
